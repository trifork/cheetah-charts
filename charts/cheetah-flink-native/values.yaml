# Default values for cheetah-flink-native.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: ""
fullnameOverride: ""

# -- Only used to decrease duplicate configuration of this chart, if image-automation is used as a sub chart.
# Overrides the local values if given
global:
  image:
    repository: ""
  imagePullSecrets: []

image:
  repository: flink
  pullPolicy: Always
  tag: "main"
  sha: ""

# -- change this to force a restart of the job,
# see https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/ for more info
restartNonce: 0

serviceAccount:
  create: true
  name: ""

rbac:
  create: true
  # -- Additional rules to add to the role
  additionalRules: []

ingress:
  # -- Whether to expose the Flink UI,
  # The UI will be exposed under https://<.ingress.domain>/<release-namespace>/<release-name> by default
  enabled: false
  certType: "staging"
  domain: flink.cheetah.trifork.dev
  ingressClassName: nginx
  annotations:
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  # -- the ui port. Ingress will hit the service on this port
  uiPort: 8081

vault:
  enabled: true
  tlsSecret: vault-tls
  serviceaccount: default

flink:
  # -- Which Flink version to use
  version: v1_15

  configuration:
    s3.path-style-access: "true"
    state.backend: "hashmap"
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    execution.checkpointing.interval: "10 minutes"
    execution.checkpointing.min-pause: "10 minutes"
    execution.checkpointing.timeout: "5 minutes"
    rest.flamegraph.enabled: "true"
    taskmanager.numberOfTaskSlots: "2"
    s3.access-key: "vault:secret/data/global/flink/s3/cheetah-flink#accessKey"
    s3.secret-key: "vault:secret/data/global/flink/s3/cheetah-flink#secretKey"
    s3.endpoint: "vault:secret/data/global/flink/s3/cheetah-flink#endpoint"

  jobManager:
    replicas: 1

    metrics:
      # -- enable metrics ports for jobManager
      enabled: true
      portName: metrics
      port: 9249

    extraPorts: []

    resource:
      # -- Memory to reserve for the Job Manager. The default value should be ok for most jobs.
      memory: 1Gb
      # -- CPU to reserve/limit for the Job Manager. Currently requests and limits are set to this value so the default value is a compromise between slowing the Job Manager down too much during startup and not reserving too much CPU unnecessarily.
      cpu: 0.1

    volumes: []

    volumeMounts: []

    podLabels: {}
    podAnnotations: {}

    # -- Any additional configuration passed to the jobmanager
    additionalConfigs: {}

    imagePullSecrets: []

    env: []

  taskManager:
    replicas: 1
    volumes: []
    volumeMounts: []

    metrics:
      # -- enable metrics ports for taskManager
      enabled: true
      portName: metrics
      port: 9249
    extraPorts: []

    resource:
      # -- Memory to reserve for each Task Manager. The amount needed depends on the amount of data to be processed by each Task Manager, and how much state it should store in memory.
      memory: 1Gb
      # -- CPU to reserve/limit for each Task Manager. Currently requests and limits are set to this value to the default value is a compromise between slowing the Task Manager down too much and not reserving too much CPU unneccesarily. This will impact the rate at which each Task Manager can process data directly. Setting it too low can also cause timeouts during checkpointing.
      cpu: 0.1

    podLabels: {}
    podAnnotations:
      # workaround to the flink-operator using pods instead of deployments
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"

    # -- Any additional configuration passed to the taskmanager
    additionalConfigs: {}

    imagePullSecrets: []

    env: []

  job:
    # -- the path of the job jar
    jarURI: ""
    # -- the name of the job class
    className: ""
    args: []

    # -- Must be either: running or suspended
    state: running

    # -- Must be either: savepoint, last_state, stateless
    upgradeMode: "savepoint"

    # -- change this to trigger a savepoint manually,
    # see more here: https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/
    savepointTriggerNonce: 0

    # -- change this to force a manual recovery checkpoint,
    # see more here: https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/
    initialSavepointPath: ""

    # -- If this is true, it will ignore the past checkpoints and start anew. Usefull if the job schema has changed.
    allowNonRestoredState: false

    # -- How many jobs to run in parallel,
    # see more here: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/parallel/
    parallelism: 2
    restartPolicy: Never
    volumes: []
    volumeMounts: []

    podLabels: {}
    podAnnotations:
      # -- Explicit disable Linkerd proxy injection, as it makes the job hang
      linkerd.io/inject: disabled

    # -- Any additional configuration passed to the job
    additionalConfigs: {}

    # -- Define the topics this job will consume
    # must be defined as follows
    # - name: <name of variable>
    #   value: <name of topic>
    # ie:
    # - name: input-kafka-topic
    #   value: "sourceTopic"
    # - name: output-kafka-topic
    #   value: "sinkTopic"
    topics: []

# -- Settings passed to the image-automation chart,
# Image-automation is not possible when using image-sha as a tagging strategy
image-automation:
  enabled: false

monitoring:
  # -- Enable monitoring. Define flinkProperties to define the monitoring properties
  enabled: true
  podTargetLabels:
    - component
    - cluster

  # -- Define which monitoring system to use,
  # See more here: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/metric_reporters/
  flinkProperties:
    metrics.reporters: prom
    metrics.reporter.prom.port: "9249"

  # -- Additional pod selector labels, which are also added to the PodMonitor labels.
  # Include labels which are selected for in the Prometheus operator
  podMonitorSelectorLabels:
    prometheus: cluster-metrics
    cheetah-monitoring: "true"

  podMetricsEndpoints:
    - port: metrics
