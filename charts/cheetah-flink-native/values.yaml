# Default values for cheetah-flink-native.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: ""
fullnameOverride: ""
secretName: &secretname "auto"

# -- Only used to decrease duplicate configuration of this chart, if image-automation is used as a sub chart.
# Overrides the local values if given
global:
  image:
    repository: ""
  imagePullSecrets: []

image:
  repository: flink
  pullPolicy: Always
  tag: "main"
  sha: ""

restartNonce:

serviceAccount:
  create: true
  name: ""

rbac:
  create: true
  # -- Additional rules to add to the role
  additionalRules: []

ingress:
  # -- Whether to expose the Flink UI
  enabled: false
  certType: "staging"
  domain: flink.cheetah.trifork.dev
  ingressClassName: nginx
  annotations:
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"

flink:
  # -- Which Flink version to use
  version: v1_14

  storage:
    secretName: *secretname
    enabled: true
    # -- If the job changes too much, the savepoints of an earlier job cannot be used.
    # The generation is added as a suffix to the savepoints directory, fixing the problem.
    # Ignored if not greater than zero
    generation: 0

  state:
    backend: hashmap

  vault:
    enabled: true
    tlsSecret: vault-tls
    serviceaccount: default

  envVars: []

  configuration:
    s3.path-style-access: "true"
    state.savepoints.dir: "s3p://flink/test-cheetah-flink-native/savepoints"
    state.checkpoints.dir: "s3p://flink/test-cheetah-flink-native/checkpoints"
    state.backend: "hashmap"
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    high-availability.storageDir: "s3p://flink/test-cheetah-flink-native/ha"
    metrics.reporter.prom.class: "org.apache.flink.metrics.prometheus.PrometheusReporter"
    metrics.reporter.prom.port: "9249"
    metrics.reporters: "prom"
    execution.checkpointing.interval: "10 minutes"
    execution.checkpointing.min-pause: "10 minutes"
    execution.checkpointing.timeout: "5 minutes"
    rest.flamegraph.enabled: "true"
    taskmanager.numberOfTaskSlots: "2"
    s3.access-key: "vault:secret/data/flink-teste/cheetah-flink-s3#accessKey"
    s3.secret-key: "vault:secret/data/flink-teste/cheetah-flink-s3#secretKey"
    s3.endpoint: "vault:secret/data/flink-teste/cheetah-flink-s3#endpoint"

  jobManager:
    replicas: 1

    # enable metrics ports for jobManager
    metrics:
      enabled: true
      extraPorts:
        - name: metrics
          containerPort: 9249

    resource:
      memory: 1Gb
      cpu: 0.5

    volumes:
      - name: s3-config
        secret:
          secretName: *secretname
          items:
            - key: test.yaml
              path: test.yaml

    volumeMounts:
      - name: s3-config
        mountPath: /opt/flink/conf/extra

    podLabels: {}
    podAnnotations: {}

    # -- Any additional configuration passed to the jobmanager
    additionalConfigs: {}

    imagePullSecrets: []

    ports:
      ui: 8081
      # rpc: 6123
      # blob: 6124
      # query: 6125

  taskManager:
    volumes: []
    volumeMounts: []

    # enable metrics ports for taskManager
    metrics:
      enabled: true
      extraPorts:
        - name: metrics
          containerPort: 9249
          protocol: TCP

    resource:
      memory: 1Gb
      cpu: 0.5

    podLabels: {}
    podAnnotations: {}

    # -- Any additional configuration passed to the taskmanager
    additionalConfigs: {}

    imagePullSecrets: []

  job:
    jarURI: ""
    className: ""
    args: []

    # -- Must be either: running or suspended
    state: running

    # -- Must be either: savepoint, last_state, stateless
    upgradeMode: "savepoint"

    savepointTriggerNonce:

    initialSavepointPath:

    allowNonRestoredState: false

    parallelism: 2
    restartPolicy: Never
    volumes: []
    volumeMounts: []

    podLabels: {}
    podAnnotations:
      # Explicit disable Linkerd proxy injection, as it makes the job hang
      linkerd.io/inject: disabled

    # -- Any additional configuration passed to the job
    additionalConfigs: {}

  flinkProperties:
    taskmanager.numberOfTaskSlots: 1
    # scheduler-mode: reactive  # Currently not working
    rest.flamegraph.enabled: true
    execution.checkpointing.interval: 10 minutes
    execution.checkpointing.timeout: 5 minutes
    execution.checkpointing.min-pause: 10 minutes

# -- Settings passed to the image-automation chart
# Image-automation is not possible when using image-sha as a tagging strategy
image-automation:
  enabled: false

monitoring:
  enabled: false
  podTargetLabels:
    - component
    - cluster

  flinkProperties:
    metrics.reporters: prom
    metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
    metrics.reporter.prom.port: 9249

  # include the podMonitorSelectorLabel which you have set in your prometheus-operator
  # set podMonitorSelectorLabels {} if your prometheus-operator is set to collect all podMonitors
  podMonitorSelectorLabels:
    prometheus: cluster-metrics
    cheetah-monitoring: "true"

  podMetricsEndpoints:
    - port: metrics
